#### 해당 TIL은 **최원영님의 아파치 카프카 애플리케이션 프로그래밍 with 자바** 책으로 공부한 내용을 정리한 글입니다.
<img src="images/아파치 카프카 애플리케이션 프로그래밍 with 자바.jpeg">

## 카프카의 탄생

### 링크드인의 고민 🤔

2011년, 링크드인은 파편화된 데이터 수집 및 분배 아키텍처를 운영하는 데에 큰 어려움을 겪었다.

데이터를 생성하고 적재하기 위해서는 데이터를 생성하는 소스 애플리케이션과 데이터가 최종 적재되는 타깃 애플리케이션을 연결해야 한다.

서비스 초기 운영 시에는 단방향 통신을 통한 소스 애플리케이션에서 타깃 애플리케이션으로의 연동과 아키텍처가 복잡하지 않았지만, 아키텍처가 거대해지고 복잡해질수록 데이터를 전송하는 라인이 매우 복잡해졌다.

소스 애플리케이션과 타깃 애플리케이션을 연결하는 파이프라인 개수가 많아지면서 소스코드 및 버전 관리에서 이슈가 생겼고, 타깃 애플리케이션에 장애가 생길 경우 그 영향이 소스 애플리케이션에 그대로 전달되었다.

이런 데이터 파이프라인의 파편화를 개선하고자 링크드인은 아파치 카프카를 만들었다.

### 카프카의 역할

카프카는 각각의 애플리케이션끼리 연결하여 데이터를 처리하지 않고 중앙집중화를 통해 한 곳에 모아 처리하고, 웹사이트, 애플리케이션, 센서 등에서 취합한 데이터 스트림을 한 곳에서 실시간으로 관리할 수 있다.

따라서, 카프카는 기업의 대용량 데이터를 수집하고 이를 사용자들이 실시간 스트림으로 소비할 수 있게 만들어주는 일종의 중추 신경으로 동작한다.

카프카를 중앙에 배치함으로써 소스 애플리케이션과 타깃 애플리케이션 사이의 의존도를 최소화하여 커플링을 완화했다.

기존 1:1 매칭으로 개발하고 운영하던 데이터 파이프라인은 커플링으로 인해 한쪽의 이슈가 다른 한쪽의 애플리케이션에 영향을 미치곤 했지만, 카프카는 이러한 의존도를 타파하였다.

소스 애플리케이션에서 생성되는 데이터는 어느 타깃 애플리케이션으로 보낼 것인지 고민하지 않고 일단 카프카로 넣으면 된다.

카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO방식의 큐 자료구조와 유사하다. 큐에 데이터를 보내는 것이 프로듀서이고 큐에서 데이터를 가져가는 것이 컨슈머다.

카프카를 통해 전달할 수 있는 데이터 포맷에는 제한이 없다. 직렬화, 역직렬화를 통해 `ByteArray`로 통신하기 때문에 자바에서 선언 가능한 모든 객체를 지원한다.

상용 환경에서 카프카는 최소 3개 이상의 서버(브로커)에서 분산 운영하여 프로듀서를 통해 전송받은 데이터를 파일 시스템에 안전하게 기록한다.

서버 3대 이상으로 이루어진 카프카 클러스터 중 일부 서버에 장애가 발생하더라도 데이터를 지속적으로 복제하기 때문에 안전하게 운영할 수 있다. 또한, 데이터를 묶음 단위로 처리하는 배치 전송을 통해 낮은 지연과 높은 데이터 처리량도 가지게 되었다.

따라서, 카프카는 엄청난 양의 데이터를 안전하고 빠르게 처리하는 강점을 가진다.

## 빅데이터 파이프라인에서 카프카의 역할

현대 IT 서비스는 디지털 정보로 기록되는 모든 것을 저장한다.

사용자가 남긴 모든 발자취가 데이터로 쌓인다.

기존 데이터베이스에서도 사용하기 편리한 스키마 기반의 정형 데이터부터 일정한 규격이나 형태를 지니지 않은 비정형 데이터까지 빅데이터로 적재되는 데이터의 종류는 다양하다.

수십 테라바이트가 넘어가는 방대한 양의 데이터를 기존의 데이터베이스로 관리하는 것은 불가능에 가깝다.

빅데이터를 저장하고 활용하기 위해서는 일단 생성되는 데이터를 모두 모으는 것이 중요하다.

이때 사용되는 개념이 데이터 레이크 `Data Lake`다.

데이터 레이크는 빅데이터를 관리하고 사용하는 측면에서 중요한 용어이다.

레이크라는 이름에서 유추할 수 있는 것처럼 데이터가 모이는 저장 공간을 뜻한다.

데이터 웨어하우스와 다르게 필터링 되거나 패키지화되지 않은 데이터가 저장된다는 점이 특징이다.

즉, 운영되는 서비스로부터 수집 가능한 모든 데이터를 모으는 것이다.

서비스에서 발생하는 데이터를 데이터 레이크로 모으려면 어떻게 해야 할까?

단순히 생각해보면 웹, 앱, 백엔드 서버, DB에서 발생하는 데이터를 데이터 레이크에 직접 엔드 투 엔드(`End to End`) 방식으로 넣을 수 있다.

서비스하는 애플리케이션 개수가 적고 트래픽이 많지 않을 때는 큰 문제가 되지 않는다.

하지만, 점점 서비스가 비대해지고 복잡해지면서 링크드인의 카프카 도입 전 사례처럼 데이터가 파편화되고 복잡도가 올라가는 문제점이 발생한다.

이를 해결하기 위해서는 데이터를 추출하고 변경, 적재하는 과정을 묶은 데이터 파이프라인을 구축해야 한다.

엔드 투 엔드 방식의 데이터 수집 및 적재를 개선하고 안정성을 추구하며, 유연하면서도 확장 가능하게 자동화한 것을 데이터 파이프라인이라고 부른다.

안정적이고 확장성이 높은 데이터 파이프라인을 구축하는 것은 빅데이터를 활용하는 기업에게 필수적이다.

데이터 파이프라인을 구축하지 않고 일회성으로 구축한 데이터 수집은 결국 데이터 흐름의 파편화로 이어진다.

이는 반복적인 유지보수를 필요로 하기 때문에 기술 부채로 남아 지속적으로 개발자들을 괴롭힌다.

이때, 데이터 파이프라인을 안정적이고 확장성 높게 운영하기 위한 좋은 방법 중 하나가 바로 아파치 카프카를 활용하는 것이다.

**아파치 카프카가 데이터 파이프라인으로 적합한 이유**는 다음과 같다.

- 높은 처리량
- 확장성
- 영속성
- 고가용성

### 높은 처리량

카프카는 프로듀서가 브로커로 데이터를 보낼 때와 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송한다.

많은 양의 데이터를 송수신할 때 맺어지는 네트워크 비용은 무시할 수 없는 규모가 된다.

동일한 양의 데이터를 보낼 때 네트워크 통신 횟수를 최소한으로 줄인다면 동일 시간 내에 더 많은 데이터를 전송할 수 있다.

많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는 데에 적합하다.

또한, 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬처리할 수 있다.

파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것이다.

### 확장성

데이터 파이프라인에서 데이터를 모을 때 데이터가 얼마나 들어올지는 예측하기 어렵다.

하루 1,000건 가량 들어오는 로그데이터라도 예상치 못한 특정 이벤트로 인해 100만 건 이상의 데이터가 들어오는 경우가 있다.

카프카는 이러한 가변적인 환경에서 안정적으로 확장 가능하드록 설계되었다.

데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃할 수 있다.

카프카의 스케일 아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원하므로 커머스나 은행 같은 비즈니스 모델에서도 안정적인 운영이 가능하다.

### 영속성

영속성이란 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성을 뜻한다.

카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다.

파일 시스템에 데이터를 적재하고 사용하는 것은 보편적으로 느리다.

하지만, 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법을 적용하였다.

운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성하여 사용한다.

페이지 캐시 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식이기 때문에 카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높은 것이다.

디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발생으로 급작스럽게 종료되더라도 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있다.

### 고가용성

3개 이상의 서버들로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다.

클러스터로 이루어진 카프카는 데이터의 복제를 통해 고가용성의 특징을 가지게 되었다.

프로듀서로 전송받은 데이터를 여러 브로커 중 1대의 브로커에만 저장하는 것이 아니라 또 다른 브로커에도 저장하는 것이다.

한 브로커에 장애가 발생하더라도 복제된 데이터가 나머지 브로커에 저장되어 있으므로 저장된 데이터를 기준으로 지속적으로 데이터 처리가 가능한 것이다.

이에 더하여 서버를 직접 운영하는 온프레미스 환경의 서버 랙 또는 또는 퍼블릭 클라우드의 리전 단위 장애에도 데이터를 안전하게 복제할 수 있는 브로커 옵션들이 준비되어 있다.

---

👀  **카프카 클러스터를 1대, 2대가 아닌 3대 이상의 브로커들로 구성해야 하는 이유**

카프카를 안전하게 운영하기 위해 최소 3대 이상의 브로커로 클러스터를 구성할 것을 추천한다.

카프카 클러스터를 구축할 때 브로커 개수의 제한은 없다.

하지만, **1대로 운영할 경우** 브로커의 장애는 서비스의 장애로 이어지므로 테스트 목적으로만 사용해야 한다.

**2대로 운영할 경우** 한 대의 브로커에 장애가 발생하더라도 나머지 한대의 브로커가 살아있으므로 안정적으로 데이터를 처리할 수 있다.

하지만, 브로커 간에 데이터가 복제되는 시간 차이로 인해 일부 데이터가 유실될 가능성이 있다.

데이터 유실을 막기 위해서 `min.insync.replicas` 옵션을 사용할 수 있다.

`min.insync.replicas` 옵션을 2로 설정하면 최소 2개 이상의 브로커에 데이터가 완전히 복제됨을 보장한다.

해당 옵션을 2로 사용할 때는 브로커를 3대 이상으로 운영해야만 한다.

왜냐하면, 3개 중 1개의 브로커에 장애가 나더라도 지속적으로 데이터를 처리할 수 있기 때문이다.

해당 옵션값보다 작은 수의 브로커가 존재할 때는 토픽에 더는 데이터를 넣을 수 없다.

그렇기 때문에 사용에서 카프라를 운영할 때는 3대 이상의 브로커로 클러스터를 구축해야 한다.

---

## 데이터 레이크 아키텍처와 카프카의 미래

데이터 레이크 아키텍처의 종류는 2가지가 있다.

- 람다 아키텍처(lambda architecture)
- 카파 아키텍처(kappa architecture)

### 람다 아키텍처

> 람다 아키텍처는 레거시 데이터 수집 플랫폼을 개선하기 위해 구성한 아키텍처이다.
>

초기 빅데이터 플랫폼은 `엔드 투 엔드`로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았다.

따라서,

- 데이터를 배치로 모으는 구조가 유연하지 않음
- 실시간으로 생성되는 데이터들에 대한 인사이트를 서비스 애플리케이션에 빠르게 전달 불가능
- 원천 데이터로부터 파생된 데이터의 히스토리 파악 어려움
- 계속되는 데이터 가공으로 인한 데이터 파편화 → 데이터 거버넌스(data governance : 데이터 표준 및 정책) 지키기 어려움

이라는 단점들을 가지고 있었다.

이를 해결하기 위해 기존의 배치 데이터를 처리하는 부분 외에 `스피드 레이어`라고 불리는 실시간 데이터 ETL작업 영역을 정의한 아키텍처를 만들었다.

람다 아키텍처는 3가지 레이어로 나뉜다.

- 배치 레이어 : 배치 데이터를 모아서 특정 시간, 타이밍마다 일괄 처리
- 서빙 레이어 : 가공된 데이터를 데이터 사용자, 서비스 애플리케이션이 사용할 수 있도록 데이터가 저장된 공간
- 스피드 레이어 : 서비스에서 생성되는 원천 데이터를 실시간으로 분석하는 용도

배치 데이터에 비해 낮은 지연으로 분석이 필요한 경우에 스피드 레이어를 통해 데이터를 분석한다.

스피드 레이어에서 가공, 분석된 실시간 데이터는 사용자 또는 서비스에서 직접 사용할 수 있지만 필요한 경우에는 서빙 레이어로 데이터를 보내서 저장하고 사용할 수 있다.

람다 아키텍처에서 카프카는 스피드 레이어에 위치한다.

서비스 애플리케이션들의 실시간 데이터를 짧은 지연시간으로 처리, 분석할 수 있기 때문이다.

카프카 스트림즈와 같은 스트림 프로세싱 도구는 윈도우 함수, 상태 기반 프로세싱, 무상태 기반 프로세싱 등 분석을 위한 다양한 기능을 제공하여 람다 아키텍처의 중요 플랫폼으로 자리잡았다.

### 람다 아키텍처의 단점

데이터를 배치 처리하는 레이어와 실시간 처리하는 레이어를 분리한 람다 아키텍처는 데이터 처리 방식을 명확히 나눌 수 있었지만, 레이어가 2개로 나뉘기 때문에 생기는 단점이 있다.

**단점**

- 데이터를 분석, 처리하는 데에 필요한 로직이 2벌로 각각의 레이어에 따로 존재해야 한다
- 배치 데이터와 실시간 데이터를 융합하여 처리할 때는 다소 유연하지 못한 파이프라인을 생성해야 한다

로직 구현체의 파편화를 해소하기 위해 1개의 로직을 추상화하여 배치 레이어와 스피드 레이어에 적용하는 형태를 고안한 “서밍버드”가 있었지만 결국 컴파일 이후에는 배치 레이어와 스피드 레이어에 각각 디버깅, 배포해야 했기에 문제가 완벽히 해결되는 것은 아니었다.

이러한 람다 아키텍처의 단점을 해소하기 위해 제이 크랩스는 `카파 아키텍처`를 제안했다.

### 카파 아키텍처

> 카파 아키텍처는 람다 아키텍처와 유사하지만 배치 레이어를 제거하고 모든 데이터를 스피드 레이어에 넣어서 처리한다는 점이 다르다.
>

람다 아키텍처에서 단점으로 부각되었던 로직의 파편화, 디버깅, 배포, 운영 분리에 대한 이슈를 제거하기 위해 배치 레이어를 제거한 카파 아키텍처는 스피드 레이어에서 데이터를 모두 처리할 수 있었으므로 엔지니어들은 더욱 효율적으로 개발과 운영에 임할 수 있게 되었다.

그런데 카파 아키텍처는 스피드 레이어에서 모든 데이터를 처리하므로 서비스에서 생성되는 모든 종류의 데이터를 스트림 처리해야 한다.

---

👀  **배치 데이터와 스트림 데이터**

> 배치 데이터는 초, 분, 시간, 일 등으로 한정된(bounded)기간 단위 데이터를 뜻한다.
>

따라서, 일괄 처리하는 것이 특징이다.

> 스트림 데이터는 한정되지 않은(unbounded) 데이터로 시작 데이터와 끝 데이터가 명확히 정해지지 않은 데이터를 뜻한다.
>

각 지점의 데이터는 보통 작은 단위로 쪼개져 있으며 웹 사용자의 클릭 로그, 사물 인터넷의 센서 데이터를 말한다.

---

### 그렇다면, 카파 아키텍처는 어떻게 배치 데이터를 스트림 프로세스로 처리할 수 있게 된 것일까?

그 배경에는 제이 크랩스가 모든 데이터를 로그로 바라보는 것에서 시작하였다.

> 로그는 일반적으로 우리가 생각하는 애플리케이션을 로깅하는 텍스트 로그가 아닌 데이터의 집합을 뜻한다.
>

이 데이터는 지속적으로 추가가 가능하며 각 데이터에는 일정한 번호(또는 타임스탬프)가 붙는다.

로그는 배치 데이터를 스트림으로 표현하기에 적합하다.

일반적으로 데이터 플랫폼에서 배치 데이터를 표현할 때는 각 시점(시간별, 일자별 등)의 전체 데이터를 백업한 스냅샷 데이터를 뜻했다.

그러나 배치 데이터를 로그로 표현할 때는 각 시점의 배치 데이터의 변환 기록을 시간 순서대로 기록함으로써 각 시점의 모든 스냅샷 데이터를 저장하지 않고도 배치 데이터를 표현할 수 있게 되었다.

로그로 배치 데이터와 스트림 데이터를 저장하고 사용하기 위해서는 변환 기록이 일정 기간동안 삭제되어서는 안되고 지속적으로 추가되어야 한다.

그리고 서비스에서 생성된 모든 데이터가 스피드 레이어에 들어오는 것을 감안하면 스피드 레이어를 구성하는 데이터 플랫폼은 `SPOF(Single Point Of Failure)`가 될 수 있으므로 반드시 **내결함성(High Availability)과 장애 허용(fault tolerant)** 특징을 지녀야 했다.

아파치 카프카는 이러한 특징에 정확히 부합하는 플랫폼이다.

카프카 내부에서 사용되는 파티션, 레코드, 오프셋은 제이 크랩스가 정의한 로그의 데이터 플랫폼 구현체로 볼 수 있다.

### 카프카의 미래

2020년 카프카 서밋에서 제이 크랩스는 카파 아키텍처에서 서빙 레이어를 제거한 아키텍처인 **스트리밍 데이터 레이크**를 제안했다.

카파 아키텍처를 살펴보면 데이터를 사용하는 고객을 위해 스트림 데이터를 서빙 레이어에 저장하는 것을 알 수 있다.

서빙 레이어는 하둡 파일 시스템, 오브젝트 스토리지와 같이 데이터 플랫폼에서 흔히 사용되는 저장소이다.

굳이 카프카를 통해 분석하고 프로세싱한 데이터를 다시 서빙 레이어의 저장소에 저장할 필요가 있을까?

스피드 레이어로 사용되는 카프카에 분석과 프로세싱을 완료한 거대한 용량의 데이터를 오랜 기간 저장하고 사용할 수 있다면 서빙 레이어는 제거되어도 된다.

오히려 서빙 레이어와 스피드 레이어가 이중으로 관리되는 운영 리소스를 줄일 수 있는 것이다.

스피드 레이어에서 데이터를 분석, 프로세싱, 저장함으로써 **단일 진실 공급원(SSOT, Single Source Of Truth)**이 되는 것이다.

데이터가 필요한 모든 고객과 서비스 애플리케이션은 스트리밍 데이터 레이크의 스피드 레이어만 참조함으로써 데이터의 중복, 비정합성과 같은 문제에서 벗어날 수 있다.

### 카프카의 개선점

아직은 카프카를 스트리밍 데이터 레이크로 사용하기 위해 개선해야하는 부분이 있다.

우선 자주 접근하지 않는 데이터를 굳이 비싼 자원(브로커의 메모리, 디스크)에 유지할 필요가 없다.

카프카 클러스터에서 자주 접근하지 않는 데이터는 오브젝트 스토리지와 같이 저렴하면서도 안전한 저장소에 옮겨 저장하고 자주 사용하는 데이터만 브로커에서 사용하는 구분 작업이 필요하다.

카프카 클러스터가 단계별 저장소를 가질 수 있도록 추가 기능을 개발하고 있다.

그리고 데이터를 사용하는 고객이나 서비스 애플리케이션에서 카프카의 데이터를 쿼리할 수 있는 주변 데이터 플랫폼이 필요하다.

컨플루언트에서는 카프카의 데이터를 SQL 기반으로 조회할 수 있도록 카프카 스트림즈를 추상화한 ksqlDB를 오픈소스로 제공하고 있다.

카프카와 ksqlDB를 사용한다면 제이 크랩스가 제안한 스트리밍 데이터 레이크에 가장 근접한 모습니다.

하지만, ksqlDB는 아직 타임스탬프, 오프셋, 파티션 기반 쿼리를 제공하지 않기 대문에 배치 데이터를 완벽히 처리하기에는 부족하다.

이외에도 아파치 스파크 스트리밍, 프레스토, 아파치 드릴, 하이브/스파크 SQL 등을 조합하는 방식도 스트리밍 데이터 레이크로 사용하는 방법이라 볼 수 있다.

하지만 모든 데이터 형태와 포맷을 지원하는 것은 아니므로 앞으로 추가적인 기능 개발이 필요하다.

카프카가 스트리밍 데이터 레이크로 완성될 수 있도록 주변 데이터 플랫폼 애플리케이션들이 개발 완료된다면, 가까운 미래에는 카프카를 사용하는 것이 데이터 레이크를 사용하는 것과 동일한 의미를 가질 것이다.